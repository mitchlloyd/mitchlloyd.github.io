---
layout: post
title:  "We're Still Programing Part 3: Why the Confusion?"
date:   2015-11-01 19:28:35 -0800
categories: jekyll update
permalink: were-still-programming-3
---

Why are people so confused about programming?
=============================================

I'd like to propose a few hypothesis for why people misunderstand programming.

First, if you just observe a programmer writing code it is hard to tell
what they are doing. An observer might not be able to tell if they are chatting
with friends, doing data entry, or inventing a compression algorithm that will
change the world. However, as more modern workers use computing devices to
perform their jobs, and have less Richard Scary jobs (Doctor, Blacksmith, Lawyer)
I feel this type of misunderstanding should pass with time.

Secondly the practice of calling programming "coding" leads to a belief that
programmers are translating one set of instructions to another.

For contrast let's look a look at Medical Coding. A patient comes into the
emergency room and their thumb has been cut off. That's ID9 code 885. Another
patient comes in needing treatment for recurring depression. That's 296.3. In
this sense, people are assigning a concept in the real world (depression) to
something that helps classify and organize data. The goal is to have a
deterministic mapping from one thing to another. Keep in mind that different
doctors may have different but valid diagnoses for conditions, but the coding
must match the diagnosis.

Let's also look at language codes. These could be secret military codes or a
code for particular mediums like Morse code. The goal here is to start with an
message like, "hello", code that word, and then get "hello" back out. It is easy
to mistake programming for a coding activity like this. If someone came to me
with a Java Program and said, I want a line-by-line replica of this program
written in Ruby, I suppose that would similar to this type of translation
work. This is also something that a programmer would not generally do. As you
might guess there are already automated or architechtural ways of dealing with
translation problems between computer languages and platforms.

Does a web developer use a series of pictures from a designer to "code" a user
interface? No. Does a programmer take a list of requirements written in English
and "code" them into Java?  No. There is no deterministic mapping between the
problems presented and the solutions delivered and, as observed by the "agile"
coding movement, the final software product is usually quite different from the
requirements -- a good thing.

Pictures aren't programs, and neither are pages of requirements. They are
representations of ideas from the designer and from the business anlayst that
need to be understood, aggregated, and reasoned about to create a new set of
ideas that produce a running program.

The word "Hello" and the translation of .... . .-..  .-.. --- are the same. If
the doctor says that your thumb was cut off, the medical coder should always
code that as 855. Any variance in these situations is a mistake. To say that a
programmer "codes" a program is like saying that Walter Isaacson "coded" Steve
Job's life into a book.

The Ethos of the Argument
=========================

I've challenged the premise that the demand for programming (or "coding") will
decrease due to new user interfaces or advances in computing. In summary,
programming has steadily become more efficient and approachable since it had a
name and the demand for the work has not decreased. As we make programming
"easier", the incidental complexity of technology fades away, but the inherent
complexity in the world's problems stays constant. Because of this, we are
likely to reach diminishing returns as we continue to improve the state of
programming.

However, I do understand the ethos of the argument against programming as an
ever-growing profession. Something _feels_ out of balance. It seems strange that
a worker would double or triple their salary by learning this "coding" skill
that can be taught in few of months.

I believe this imbalance exists because those that pay for programming work
rarely understand its value and costs. I see companies spend millions on
programming efforts that are invisible to them. For instance, consider a company
where an employee decides to try an emerging database technology they read about
on Hacker News. Three years later the company still has a team of developers
trying to migrate to a more proven solution while others deliver features at a
hobbled pace. Another painful example of this is when novice programmers believe
they can recreate established infrastructure better than those before them while
simultaneously building a product. Management wanted a online store but they got
a failed JavaScript framework instead.

Programmers in high-pressure environments often feel they have no time to
improve process. Trade-offs that save them an hour today, cost a week of
productivity in the next month, creating in a vicious cycle. Management is in no
position to understand these trade-offs and these programmers are too uncertain
to realize that they must make resist this pressure and make better decisions.

In what other industry could the same product cost $10,000, $10,000,000, or be
impossible based on factors that management isn't even aware of? There is a
saying that software projects don't fail for "technological reasons" but in
practice it's impossible for management to tell why they fail. When all is said
and done, the costs seems as if they were inevitable, but they were not.
Programming is a black box. Building the right thing is the most
important part of a software project, but the difference between a product that
can adapt in a couple weeks vs a couple years will make or break a company.

There is hope, and as fun and wacky as new tech salaries are, we'll have
balance. The next phase for software development...

